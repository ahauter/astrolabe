llama-server -hf Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:Q5_K_M -ngl 99 -t 28 -dev CUDA0 -c 2048 -n 256
